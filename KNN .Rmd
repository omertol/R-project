---
title: "KNN"
output:
  word_document: default
  html_document: default
  pdf_document: default
date: '2022-06-06'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load-packages, message = FALSE}
library(knitr)
library(tidyverse)
library(broom)
library(htmltools)
library(class)

```

```{r}
#Load data
list_df <- list.files("./data/spike data", pattern="*.csv", full.names=TRUE)
ldf <- lapply(list_df, read.csv)
```

```{r find wells and treatments, warning=FALSE}
if (file.exists("./data/wells and treatment.csv")){
  df <- read.csv("./data/wells and treatment.csv")
} else {
  # Load one of the data files (arbitrarily) to extract the well information chart
  well_treat <- read.csv(file = "./data/spike data/ac div16(000)_spike_list.csv")
  
  # Find the chart in the file
  rnom <- as.numeric(which(well_treat == "Well Information"))
  well_treat <- fread(file = "./data/spike data/ac div16(000)_spike_list.csv", skip=rnom)
  
  
  # Select only well ID and treatment given
  well_treat <- well_treat[which(well_treat$`Well Information` == "Well" | 
            well_treat$`Well Information` == "Treatment"),]
  
  # Transpose to a longer data frame and fix labeling
  well_treat <- data.frame(t(well_treat[,2:ncol(well_treat)]))
  rownames(well_treat) <- seq(1:nrow(well_treat))
  colnames(well_treat) <- c("Well", "Treatment")
  
  # Filter inactive wells
  well_treat <- well_treat[well_treat$Treatment != "Inactive",]
  
  # Create and save a .csv file for future use
  write.csv(well_treat,"./data/wells and treatment.csv", row.names = FALSE)
}
```

```{r}
get_petri <- function(df){
  df_res <- df[c(3,4,5)]
  df_res <- separate(df_res, col = "Electrode",
           into = c("Petri", "Electrode"),
           sep = "_")
  df_res <- drop_na(df_res)
  return(df_res)
}

filter_data<- function(data){
  #only the first 10 minutes
  data<- data %>% filter(data$Time..s.<=600)
  data <- data %>% count(Petri)
  return(data)
}


ldf <- lapply(ldf, get_petri)
ldf <- lapply(ldf, filter_data)

```

```{r}
df1 <- ldf[1]
ldf <- ldf[2:length(ldf)]
```

```{r}
get_df <- function(df1, df2){
  
  df1 <- df1 %>% data.frame()
  df2 <- df2 %>% data.frame() 
  
  #create one df
  tmp <- right_join(df1,df,by="Petri") 
  tmp <- right_join(df2,tmp, by="Petri")
  tmp[is.na(tmp)] <- 0 #replace na`s value with 0's
  tmp$n <- as.numeric(tmp$n.y - tmp$n.x)
  return(tmp[, c("Petri", "n", "Treatment")])

}

pairwise_knn <- function(join_df){
  join_df <- join_df[,c("n", "Treatment")]
  ran <- sample(1:nrow(join_df), 0.8 * nrow(join_df))
  nor <-function(x) {
    return ((x -min(x))/(max(x)-min(x)))
    }
    
  ##Run normalization on first column of dataset because it is the predictor
  join_df_norm <- as.data.frame(lapply(join_df[c(1)], nor))
  train <- data.frame(join_df_norm[ran,])
  test <- data.frame(join_df_norm[-ran,])
  target<-as.factor(join_df[ran,2])
  test_category <-as.factor( join_df[-ran,2])
  
  knn_model <- knn(train=train,test=test,cl=target, k=10)
  
  ##create confusion matrix
  tab <- table(knn_model,test_category)
  ##this function divides the correct predictions by total number of predictions that tell us how accurate teh model is.
  accuracy <- function(x){
    return(sum(diag(x)/(sum(rowSums(x)))) * 100)
  }
  
  print(accuracy(tab))
}

```

```{r}
for(i in  ldf){
  join_df <- get_df(df1,i)
  df1 <- i
  pairwise_knn(join_df)
}

```








